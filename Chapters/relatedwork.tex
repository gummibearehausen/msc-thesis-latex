\chapter{Related Work}
\label{rw:intro}

In this chapter relevant work related to the thesis topic will be briefly presented. In section
\ref{rw:logicProgramming}...


\section{Logic Programming}
\label{rw:logicProgramming}

\cite{DBLP:books/sp/Lloyd87}
\cite{DBLP:journals/ml/LavracD96}
In this section, basic logic programming and deductive database terminology such as \emph{literal}, \emph{clause},
\emph{program clause}, \emph{datalog clause} and \emph{hypothesis} will be presented. Firstly, it is important to
mention that variables are represented as uppercase letters followed by a string of lowercase letters and/or digits.
Function and predicate symbols are lowercase letters also followed by a string of lowercase and/or digits.

An \emph{atomic formula} $L$ is a predicate symbol followed by a bracketed n-tuple of \emph{terms}. A \emph{term} can be
a variable or a function symbol followed by a bracketed n-tuple of terms. A constant is a function symbol of arity 0. So
for example, if $f$, $g$, and $h$ are function symbols and $X$ a variable, then $g(X)$ is term, $f(g(X),h)$ is also a
term and $h$ is a \emph{constant}.

A \emph{literal} is an \emph{atomic formula} which can be negated or not. So both $L$ and its negation $\overline{L}$
are literals for any \emph{atomic formula} $L$. A clause $c$ is a disjunction of literals, for example:
\begin{center}
  $c=(L_1 \vee L_2 \vee \ldots \overline{L_{i}} \vee \overline{L_{i+1}} \vee \ldots) \equiv
 L_1 \vee L_2 \vee \ldots \leftarrow L_i \wedge L_{i+1} \wedge \ldots$
\end{center}

Such disjunction of literals can also be written in following way:
\begin{center}
 $\{L_1,L_2,\ldots,\overline{L_i},\overline{L_{i+1}},\ldots\}$ \\
$ L_1,L_2,\ldots \leftarrow L_i,L_{i+1},\ldots$
\end{center}

A \emph{program clause} is a clause which contains exactly one positive literal. That is, it has the form:
\begin{center}
 $\underbrace{T}_{head} \leftarrow \underbrace{L_1,L_2,\ldots}_{body}$
\end{center}

A \emph{datalog clause} is a program clause with no function symbols with arity greater different from zero. That means
that only variables and constants can be used as predicate arguments. A datalog clause is considered $safe$ if all the
variables present in the head literal $T$ are also present in the body. Moreover, it may also allow negated literals in
the body, as long as every variable existent in a negated body literal are also be present in a non-negated body
literal.

\section{Inductive Logic Programming}

\cite{DBLP:journals/ml/LavracD96}
Inductive Logic Programming (ILP) is a machine learning technique that combines inductive machine learning with the
logic programming representation. Given a known background knowledge and a set of training examples represented as a
logical database of facts (literals without any variables), an ILP system will learn a hypothesis in the form of a
logic program.

The training data is a set of examples $\varepsilon$, where each examples is a grounded fact labeled as positive
($\oplus$) or negative ($\ominus$). We denote the set of positive examples as $\varepsilon^{+}$ and the set of negative
examples as $\varepsilon^{-}$

The background knowledge $\mathcal{B}$ is a prior knowledge which contributes in learning the hypothesis. It indirectly
restricts the hypothesis search space, as  the learned hypothesis $\mathcal{H}$ should be consistent with the background
knowledge as well as the training examples.

A hypothesis $\mathcal{H}$ is said to cover an example $e$ given a background knowledge $\mathcal{B}$
($covers(\mathcal{B} \cup \mathcal{H},e)$) if the example satisfies the hypothesis and background knowledge. In logic
programming where the a $\mathcal{H}$ is a set of program clauses and an example is a ground fact, it means that $e$ is
entailed by $\mathcal{B} \cup \mathcal{H}$.

\begin{center}
 $covers(\mathcal{B} \cup \mathcal{H},e)=true$ if $\mathcal{B} \cup \mathcal{H} \models e$
\end{center}

We can also define a covering function for a set of examples which returns a a subset with the examples entailed by
$\mathcal{B} \cup \mathcal{H}$:

\begin{center}
 $covers(\mathcal{B} \cup \mathcal{H},\varepsilon)=\{e \in \varepsilon | \mathcal{B} \cup \mathcal{H} \models e\}$
\end{center}

Therewith, we can define two important concepts in inductive learning:

\begin{itemize}
 \item Completeness: A hypothesis $\mathcal{H}$ is complete with respect to background knowledge $\mathcal{B}$ and
examples $\varepsilon$ if all the positive examples are covered, or in other words:
$covers(\mathcal{B},\mathcal{H},\varepsilon^{+})=\varepsilon^{+}$
 \item Consistency: A hypothesis $\mathcal{H}$ is consistent with respect to background knowledge $\mathcal{B}$ and
examples $\varepsilon$ if no negative examples are covered, or in other words:
$covers(\mathcal{B},\mathcal{H},\varepsilon^{-})=\emptyset$
\end{itemize}

The objective of ILP is to find a hypothesis that is complete and consistent with respect to the given training
examples and background knowledge. The example shown in table \ref{tb:ilpExample} (presented in
\cite{DBLP:journals/ml/LavracD96})
illustrates a simple problem of learning a hypothesis for the target relation $daughter(X,Y)$.

\begin{center}
    \begin{table}
    \label{tb:ilpExample}
    \caption{A simple ILP problem: learning the \emph{daughter} relation.}
      \begin{tabular}{ r | l }
      \toprule
      \textbf{Training Examples} & \textbf{Background Knowledge}\\
      \midrule
      daughter(mary,ann) $\oplus$	& parent(ann,mary).	\\
      daughter(eve,tom) $\oplus$	& parent(ann,tom).	\\
      daughter(tom,ann) $\ominus$ 	& parent(tom,eve).	\\
      daughter(eve,ann) $\ominus$	& parent(tom,ian).	\\
					& female(ann).		\\
					& female(mary).		\\
					& female(eve).		\\
      \bottomrule
      \end{tabular}
    \end{table}
\end{center}

If we consider, for example, the language of safe datalog clauses, it is possible to formulate the following complete
and consistent hypothesis:

\begin{center}
  $hypothesis = daughter(X,Y)$ :- $female(X),parent(Y,X)$ 
\end{center}

\subsection{Searching the Hypothesis Space}

In ILP, the hypothesis space is determined by the language of the programs $\mathcal{L}$ consisting of the possible
program clauses allowed by the language. Also, the vocabulary of predicate symbols is determined by the predicates from
the background knowledge $\mathcal{B}$.

It is useful to structure the search space with partial ordering into a set of clauses based on $\theta$-subsumption in
order to systematically search the program clauses space. A clause $c$ $\theta$-subsumes $c'$ if there's a
substitution $\theta$ such that clause $c\theta \subseteq c'$. This also introduces a notion of generality, where
clause $c$ is at least as generaly as $c'$, or in other words, $c'$ is a specialization of $c$.

For example, for $c=daughter(X,Y)$:-$parent(Y,X)$ and $c'=daughter(X,Y)$:-$female(X),parent(Y,X)$. We know that $c'$ is
a specialization of $c$ because for $\theta=\emptyset$, $c \subset c'$ since $\{daughter(X,Y),\neg parent(Y,X)\}
\subset \{daughter(X,Y),\neg female(X),\neg parent(Y,X)\}$.

If we have $c = livesIn(X,Y)$:-$marriedTo(X,Z),livesIn(Z,Y)$ and $c' =
livesIn(X,germany)$:-$marriedTo(X,Z),livesIn(Z,germany)$, we know that $c'$ is a specialization of $c$ because for the
substitution $\theta=\{Y/germany\}$, $c\theta=c'$.

This notion gives us an important property that cab be used for pruning parts of the search space:

\begin{itemize}
 \item When generaling from $c$ to $c'$, all the examples covered by $c$ are also covered by $c'$. So if $c$ is
inconsistent, then all its generalizations are inconsistent as well.
  \item When specializing from $c$ to $c'$, an example not covered by $c$ will neither be covered by $c'$. So if $c'$
does not cover any positive examples, neither do all its specializations.
\end{itemize}

These properties are the basis for two main search approaches:

\begin{itemize}
 \item Bottom-up: starts with less general clauses and searches least general generalizations
 \item Top-down: starts with more general clauses and searches for most general consistent specializations
\end{itemize}

As in this thesis we work only with the top-down approach, we will discuss it in more details in the next subsection.
Further information about bottow-up approach can be found in \cite{DBLP:journals/ml/LavracD96}.

\subsection{Top-Down ILP}

Top-down ILP algorithm relies on specializations. Given the $\theta$-subsumtion definition, we have two main
specialization techniques:

\begin{itemize}
 \item Apply a substitution on the clause
 \item Add a literal to the body of the clause
\end{itemize}

The top-down algorithm consists basically of a specitialization loop that refines a clause ensuring consistency
embedded inside a covering loop that adds clauses to the hypothesis ensuring completeness. In domains with perfect data,
the stopping criterea require that all positive examples are covered (completeness) and no negative examples are
covered (consistency). Nevertheless, in domains with imperfect data (due to noise, incompleteness, etc.), heuristic
stopping criterea can be used to tolerate some level of incompleteness and inconsistency.

This simplest heuristic is the expected accuracy of a clause, which is defined as the probability that an example
covered by the clause is labeled as positive:
\begin{equation}
A(c)=P(e in \varepsilon^{+}|c)=\cfrac{n^{+}(c)}{n^{+}(c)+n^{-}(c)} 
\end{equation}
where $n^{+}(c)$ is the number of postive and $n^{-}(c)$ the number of negative examples covered by $c$. 

[Sampling,MaxLiterals]


\section{First Order Inductive Learning}
\cite{DBLP:journals/ml/Quinlan9}
\cite{DBLP:conf/ecml/QuinlanC93}

\section{Association Rule Mining}
\cite{Agrawal:1993:MAR:170036.170072}

Association Rule Mining is method for discovering interesting relations between variables in large databases. It is
intended to work on a database composed by a set of transactions $D=\{t_1,t_2,\ldots,t_m\}$. Each transaction consists
of a set of items $I=\{i_1,i_2,\ldots,i_n\}$, which contains $n$ binary attributes indicating the presence or
absence of each item in the transaction.

The objective of association rule mining is to learn inference rules of the form $X \rightarrow Y$, where 



\subsection{Measures}
\subsubsection{Support}
\subsubsection{Confidence}
\subsubsection{Lift}
\subsection{Itemset Lattice}
\subsection{Apriori Algorithm}

\section{Mining Optimized Rules for Numeric Attributes}

\cite{Brin99miningoptimized}

\section{Information Theoretic Measures}

\cite{DBLP:conf/sac/CaldersGPR09}

\section{Semantic Web Applications}
\section{Linked Open Data Applications}