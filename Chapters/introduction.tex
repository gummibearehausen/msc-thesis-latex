\chapter{Introduction}
\label{ch:intro}

Public archives preserve the collective memory of societies. Their importance
in recording history, documenting the progress of civilization and
preserving cultural heritage are well established now. The practice of
archiving predates the era of digital technology and archives are known to have
existed for many centuries. Such archives have been maintained by different
institutions like universities, libraries, governments, or corporations. Of the
many ways in which the printing press changed the ways of our societies, the
ease with which archiving could be done can be considered one. Because printed
material could be produced frequently and in large numbers, archives began to
grow in size and nature. 

The development of the World Wide Web towards the end of the previous century
can be considered another landmark event, whose ramifications on different
aspects of human civilization are still unfolding. The possibilities offered by
the World Wide Web are being incorporated by the society in a variety of
ways, as both continue to evolve. As a result, the process of publishing and
archiving have also undergone a transformation. The process of publication,
revision, storage, and distribution has become a lot easier and quicker. While
decreasing storage costs have resulted in an explosive growth in the number of
documents in the World Wide Web, these documents change rapidly and may
disappear altogether after some time. On the other hand, because of the same
reasons, digitally produced material is also easier to save and store for a
long time. Thus despite being highly volatile, digital material is easy to
preserve. 

An increased awareness for the need to preserve born-digital content has resulted in
initiatives to archive the Web. Similarly, documents produced outside the
internet are also being digitized and archived. The oldest archive of web-born
content, the Internet Archive was founded in 1996. It has collected over 150
billion web pages. Other specialized web archives are also maintained by
governmental agencies, libraries, and corporations. There are about 13 million
documents with the earliest one from 1851 in the archive of the New York Times.
The archive of the Gale NewsVault~\cite{Gale}, which provides access to the archives of 
British publications like The Times, The Economist and the Financial Times
contains over 10 million pages and 400 years of content. The last two examples
are of documents that were published originally in analog form, but were
subsequently converted to and archived in digital format.

In this work, we consider both of the above mentioned types of archives as
\emph{web archives}. Additionally, our work deals only with the text documents
in the archives.

The volume of data in a typical web archive is huge and it keeps increasing.
Through successive addition of resources, the archives gather new documents or
newer versions of existing documents. Merely preserving all the documents ever
seen is not enough for the different purposes a web archive is supposed to
serve. Take for example, the different use-case scenarios proposed by the
International Internet Preservation Consortium (IIPC) \cite{IIPC:2006}. Some of
these use cases require further processing of the archive data to facilitate
advanced search and analysis. 

A web archive may contain many redundant documents. Redundancy is present in
archives for various reasons including partial or complete copying of
content, change of address or mirroring. Some documents could be duplicates of
others. Additionally, other forms of redundancies may exist because of the
evolution of the content in a document. Documents on the Web keep changing
because of minor modifications like spelling and grammatical corrections or
because of more significant changes in the content. After a document is created,
it may undergo a series of modifications resulting in newer versions. Thus,
many versions of a document may be stored in a web archive and some of them
might be redundant. 

Redundant content wastes storage and makes the processing of the collection
difficult and inefficient. For instance, a keyword search on a collection with low redundancy
would produce results from more diverse sources than on a collection with high
redundancy. If a web archive cannot be processed and searched efficiently or if
the search results are not useful to the user, not only will the usability of
the archive be low, society will not be able to benefit from the treasured
information. For these reasons, merely creating a web archive is not sufficient.
The task of making them more usable for the users is of equal importance.  If
redundant documents could be removed from the collection, the size of the
document collection would decrease, making the processing of the collection
easier.

\section{Motivation}

Consider the following use cases as a motivation for the work presented in the
following chapters:

\paragraph{Use Case \#1:} A historian wants to gather references for her study. She
wants to study the documents related to \emph{parliamentary affairs}. Many of the
documents are no longer available online and she needs to use a web archive to
find the documents. Upon searching for documents in the archive, she is
presented with a large number of documents present in the archive. The archive
has several versions of each document. It is necessary to identify the documents
whose content is redundant and present diverse results to the user.

\paragraph{Use Case \#2:} A journalist wants a group of representative documents from
the archive based on his needs. While working on a story, he wants to find all
documents from the BBC that mention ``London Olympics'' and were published in
2010. The archive has many redundant documents and there is a need to generate a
set of representative documents matching the journalist's needs. However, a
typical duplicate detection system may mark all documents from 2010 as redundant
because of documents in 2011 or 2012 containing similar content. This is not
what the user wants as he is interested only in representative documents from
2010. There is a need to provide a solution suitable to the user's needs.

\paragraph{Use Case \#3:} A researcher working in an archiving agency has to
regularly run analytical jobs on the content of the archives. The nature of most
of the jobs is to find out what topics, time period, or content are covered by
the documents of the archive. Because of a lot of redundant documents, these
analytical jobs take a lot of time to complete. The archivist could produce
reports quickly if his jobs didn't have to spend time on documents that are
redundant. To get an overview of the nature of the archive, removal of redundant
documents would be a desired functionality.

For these reasons, it is important to control redundancy in web archives. For
many use cases, it is not enough to merely apply a duplicate detection system as
the user requirements are varied. The definition of redundancy may change from
one user to another and depends on the specific task at hand. We aim to decouple
the definition of redundancy from the actual method used to control it. Allowing
the users to define redundancy in their own terms and employing the user-defined
redundancy measure is the main aim of our work. In this work, we closely inspect
the nature of web archives and the challenges in processing them. 

\section{Contributions}
In order to make a web archive serve its many purposes, it is imperative to
find efficient ways to control redundancy. This work makes the following
contributions towards making this possible:

\begin{enumerate}

  \item a framework that allows users to define redundancy based on (mutual)
    containment of document contents and meta data constraints,
  \item an approach that determines for a user-defined notion of redundancy a
    small set of documents representative for the collection,
  \item efficient algorithms to implement the approach on top of modern
    platforms for distributed data processing and management, specifically,
    Hadoop and HBase,
  \item a end-to-end system prototype that can take a web archive document
    collection as input and produce a collection with redundancy removed.

    \begin{comment}
\item use existing measures and methods to define if a document is ``redundant"
  or ``covered."
\item propose an efficient method to find the set of all redundant documents
  and the set of all representative documents in a collection.
\item use existing frameworks to run the method proposed above in a distributed
  computing environment.
\item propose an end-to-end system prototype that can take a web archive
  document collection and produce a collection with redundancies removed.

  \end{comment}
\end{enumerate}

\section{Outline}

The remainder of this thesis is structured as follows. In
Chapter~\ref{ch:technical_background}, we provide technical background on
MapReduce and BigTable. In Chapter~\ref{ch:related_work}, we present a
summary of previous work in the areas of duplicate and near-duplicate detection,
information retrieval on web archives, and MapReduce applications in graph
processing. Following that, we state our problem and describe solutions in
Chapter~\ref{ch:redundancy_control}. In Chapter~\ref{ch:mapreduce_impl}, we
describe an implementation of our solution using the MapReduce framework. In
Chapter~\ref{ch:experiments}, we present our experimental results. We conclude
this thesis and outline directions of future research in Chapter~\ref{ch:future_work}.

%Plan:

%1. \\
%- web archives, what are they, who maintains them \\
%- Volume of data (stats) \\
%- Crawls \\
%- Duplicates \\
%- Redundancies \\

%2.  \\
%- Use cases \\
%- save storage \\
%- search diversity \\
%- processing of smaller vs original collection \\
%- a hostname, timestamp, topic or meta data based redundancy control \\
%- updates \\
%- re-crawl freq \\

%3. \\
%- Problems with general and specific approaches \\
%- scale, big data \\

%4. \\
%- previous work (cite) shows 25\% of all content is redundant \\
%- duplicate detection \\
%- in web archives, redundancies are not duplicates alone (give examples, how?) \\

%5. \\
%- pairwise computation, quadratic \\
%- distributed framework \\

%6. \\
%not possible to do on a single machine \\
%- we use coverage relation \\
%- we construct document graph \\
%- we partition document graph \\
%- we formulate Set Cover problem \\
%- we test keyword search and show that our methods are effective \\

