\chapter{Introduction}
\label{ch:intro}

In the last years, the volume of semantic data available, in particular RDF, has dramatically increased. Initiatives like the W3C Semantic Web, which provides a common standard that allows data to be shared and reused across different applications, and the Linked Open Data, which provides linkages between different datasets that were not originally interconnected, have great contribution in such development. Moreover, advances in information extraction have also made strong contribution, by crawling multiple non-structured resources in the Web and extracting RDF facts.

Nevertheless, information extraction still has its limitations and many of sources might contain contradictory or uncertain information. Therefore, many of the extracted datasets suffer from incompleteness, noise and uncertainty.

In order to reduce such problems, one can apply to the knowledge base a set of inference rules that describes its domain. With that, it's possible to resolve contradictions or strengthen or weaken their confidence values. It's also possible to derive new facts that are originally not existent due to incompleteness. Such inference rules can be of two types:

\begin{enumerate}

 \item \emph{Hard Rules}: Consistency constraints which represent which might represent functional dependencies, functional or inverse-functional properties of predicates or Mutual exclusion. For example:
    \begin{center}
      \emph{marriedTo(x,y) :- marriedTo(y,x)}
      \emph{grandChildOf(x,y) :- childOf(x,z)childOf(z,y)}
    \end{center}

 \item \emph{Soft Rules}: Weighted rules that frequently, but not always hold in the real world. As they might also produce incorrect information, each rule itself must have a confidence value which should be applied to derived facts, for example married people live in the same place as their partner has confidence 0.8:
    \begin{center}
      \emph{livesIn(x,y) :- marriedTo(x,z)livesIn(z,y)} [0.8]
    \end{center}
\end{enumerate}

So, if we have an incomplete knowledge base, which lacks information about where \emph{Michelle Obama}l lives, but we know that she's married to \emph{Barack Obama} and he lives in \emph{Washington, D.C.}, both with confidence 1, we could then apply this soft rule to derive the fact \emph{livesIn(MichelleObama, WashingtonDC)} with confidence 0.9.

Such rules are rarely known beforehand, or are too expensive to be manually extracted. Nevertheless, the data itself can be used to mine these rules using \emph{Inductive Logic Programming (ILP)}. 

http://people.csail.mit.edu/kersting/profile/PROFILE_ilp.html

ILP is a well-established framework for inductively learning relational descriptions (in the form of logic programs) from examples and background knowledge. Given a logical database of facts, an ILP system will generate hypothesis in a pre-determined order and test them against the examples. However in a large knowledge base, ILP becomes too expensive as the search space grows combinatorially with the knowledge base size and the larger the number of examples, the more expensive it is to test each of the hypothesis.

rules with constants might be really interesting.


Moreover, testing hypothesis with constants increases the search space dramatically, making it unfeasible to test all possible hypothesis with constants in a large knowledge base. In such case, it's necessary to arbitrarily reduce the search by restricting the set of constants to be included in ...

data mining, rule mining, datalog rules



talk a bit about ilp

\section{Motivation}
Given the huge size of search space and the great interestingness of rules with constants, we need to smartly prune constants or combinations of constants that  , learning datalog rules can be already extremely costly.

Numerical constants are a special case. They 

\section{Contributions}
In order to make a web archive serve its many purposes, it is imperative to
find efficient ways to control redundancy. This work makes the following
contributions towards making this possible:

\begin{enumerate}

  \item a framework that allows users to define redundancy based on (mutual)
    containment of document contents and meta data constraints,
  \item an approach that determines for a user-defined notion of redundancy a
    small set of documents representative for the collection,
  \item efficient algorithms to implement the approach on top of modern
    platforms for distributed data processing and management, specifically,
    Hadoop and HBase,
  \item a end-to-end system prototype that can take a web archive document
    collection as input and produce a collection with redundancy removed.

    \begin{comment}
\item use existing measures and methods to define if a document is ``redundant"
  or ``covered."
\item propose an efficient method to find the set of all redundant documents
  and the set of all representative documents in a collection.
\item use existing frameworks to run the method proposed above in a distributed
  computing environment.
\item propose an end-to-end system prototype that can take a web archive
  document collection and produce a collection with redundancies removed.

  \end{comment}
\end{enumerate}

\section{Outline}

The remainder of this thesis is structured as follows. In
Chapter~\ref{ch:technical_background}, we provide technical background on
MapReduce and BigTable. In Chapter~\ref{ch:related_work}, we present a
summary of previous work in the areas of duplicate and near-duplicate detection,
information retrieval on web archives, and MapReduce applications in graph
processing. Following that, we state our problem and describe solutions in
Chapter~\ref{ch:redundancy_control}. In Chapter~\ref{ch:mapreduce_impl}, we
describe an implementation of our solution using the MapReduce framework. In
Chapter~\ref{ch:experiments}, we present our experimental results. We conclude
this thesis and outline directions of future research in Chapter~\ref{ch:future_work}.

%Plan:

%1. \\
%- web archives, what are they, who maintains them \\
%- Volume of data (stats) \\
%- Crawls \\
%- Duplicates \\
%- Redundancies \\

%2.  \\
%- Use cases \\
%- save storage \\
%- search diversity \\
%- processing of smaller vs original collection \\
%- a hostname, timestamp, topic or meta data based redundancy control \\
%- updates \\
%- re-crawl freq \\

%3. \\
%- Problems with general and specific approaches \\
%- scale, big data \\

%4. \\
%- previous work (cite) shows 25\% of all content is redundant \\
%- duplicate detection \\
%- in web archives, redundancies are not duplicates alone (give examples, how?) \\

%5. \\
%- pairwise computation, quadratic \\
%- distributed framework \\

%6. \\
%not possible to do on a single machine \\
%- we use coverage relation \\
%- we construct document graph \\
%- we partition document graph \\
%- we formulate Set Cover problem \\
%- we test keyword search and show that our methods are effective \\

