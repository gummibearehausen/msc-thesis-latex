\chapter{Algorithmic Framework}
\label{af:intro}

\section{Knowledge Base Backend}

For storing and retrieving RDF data we use RDF3X \cite{Neumann:2010:RES:1731351.1731354}. It has the advantage of being specialized and optimized for RDF data, using a strong indexing approach with compressed B$^+$-Tree indices for each of six permutations of \emph{subject (S)}, \emph{predicate (P)} and \emph{object (O)}: \emph{SPO},\emph{SOP},\emph{OSP},\emph{OPS},\emph{PSO} and \emph{POS}.

RDF3X particularities...

\section{Preprocessing}

\subsection{\graphname}

\subsubsection{Graph Node}

Every node essentially contains the following attributes:

\begin{itemize}
 \item Set of pointers to parent nodes
 \item Set of pointers to child nodes
 \item Set of pointers to constant nodes
 \item Histogram with facts distribution over root numerical property
\end{itemize}


\subsubsection{Building the Correlation Lattice}

For building the \graphname, we start with the root node, which has a numerical property as literal and no constants assigned, e.g. \emph{hasIncome(x,y)}. We then query the  distribution of positive examples over the property in the whole Knowledge Base.

\begin{center}
 \emph{SELECT COUNT ?y WHERE \{ ?x <hasIncome> ?y \} GROUP BY (?y)}
\end{center}

It's also necessary to specify the bucketing technique and the number of buckets in order to extract the histogram from the obtained query results. These buckets are used to build the histograms of all nodes in the graph.

Afterwards, we select the the categorical properties that will be used in the lattice. For each of the selected properties, we join them with the root numerical property (for simplicity we'll assume all the categorical properties are joined with both \ord{1} arguments) and we query the distribution again. In the first level, it's necessary to extract a histogram for each of the categorical constants in the selected properties. Therefore, it's a good strategy to group the results also by these categorical constants so If we select \emph{hasEducation} for example, we would then fire the following SPARQL query:

\begin{center}
 \emph{SELECT COUNT ?z ?y WHERE \{ ?x <hasIncome> ?y . ?x <hasEducation> ?z \} GROUP BY (?z,?y)}
\end{center}

With such query, it's possible to extract a histogram for the node \emph{hasIncome(x,y)hasEducation(x,z)} and its correspondent constants. 

\subsubsection{Searching Rules in Correlation Lattice}

In the \graphname itself, it's possible to extract valuable rules. Given its characteristic of all the relations being joined on the same root argument, those rules represent how different categories are related along root's numberical constants.

For every non-root node in the \graphname, any of its parents can be seen as rule's body and the remaining literal as head, e.g.:

Let's denote $a_i$ as a relation $a(x,y)$ with constants $A_i$ for $y$, so for example $a_1 \equiv a(x,A_1)$:

For the node $r a_1 b_1 c_1$ with parents $r a_1 b_1$, $r a_1 c_1$ and $r b_1 c_1$, we can extract and easily evaluate three rules:

\begin{math}
Rule_1 : \quad a_1 \leftarrow b_1 c_1 r \\ 
\quad supp_i(Rule_1) = h_i(r a_1 b_1 c_1) \\
\quad \quad acc_i(Rule_1) = \frac{h_i(r a_1 b_1 c_1)}{h_i(r b_1 c_1)} \\ \\
Rule_2 : \quad b_1 \leftarrow a_1 c_1 r \\
\; supp_i(Rule_2) = h_i(r a_1 b_1 c_1) \\
\; acc_i(Rule_2) = \frac{h_i(r a_1 b_1 c_1)}{h_i(r a_1 c_1)} \\ \\
Rule_3 : \quad c_1 \leftarrow a_1 b_1 r \\
\; supp_i(Rule_3) = h_i(r a_1 b_1 c_1) \\
\; acc_i(Rule_3) = \frac{h_i(r a_1 b_1 c_1)}{h_i(r a_1 b_1)} \\
\end{math}

Subsequently we can analyze the frequency and confidence distributions and determine whether any of the rules are
interesting, using any of the techniques discussed in [???] and find possible interesting intervals.

\subsubsection{}

% Substitution for constants only in categorical relations, query by support
% When categorical joined to numerical property (without range) existent in \graphname query graph for constants
% 



