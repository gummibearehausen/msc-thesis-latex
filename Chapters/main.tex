\chapter{Learning Rules With Numerical and Categorical Attributes}
\label{cl:intro}

In this chapter, we will discuss what kinds of rules we are interested in, define what categorical properties are,
describe in more details a correlation lattice, how to build it and integrate it into the core ILP learning
algorithm.

\section{Interesting Rule Definition}

In this section, we formally define what kind of rules we are interested in obtaining with the algorithm proposed in
this thesis. As briefly explained in the introduction, the objective is to learn rules with numerical properties,
focusing on searching ranges in the numerical attribute's domain that satisfy support and confidence thresholds.

In this context, we define a rule with free numerical variable as \emph{base-rule} and the same rule with the
numerical variable restricted to a specific interval or value as a \emph{refined-rule}. For instance, if we have
two rules $r_1$ and $r_2$:

\begin{align*}
r_1: employmentStatus(X,unemployed)&\text{ :- }hasIncome(X,Y)
r_2: employmentStatus(X,unemployed)&\text{ :- }hasIncome(X,Y),Y>100000
\end{align*}

Then we say that $r_1$ is the base-rule of $r_2$ and $r_2$ is a refined-rule of $r_1$.

Searching numerical intervals for a base-rule that already satisfies confidence threshold is not so interesting. The
base-rule itself implicitly specifies an interval which covers the whole numerical attribute domain from any possible
refined-rules. Moreover, even if for some specific range we have a higher confidence value, the gain in comparison to
its base-rule would not be very significant as the base-rule already presents a high confidence value (at least higher
than the specified threshold).

More precisely, if we have a confidence threshold $TS_{conf}$, and define confidence gain from a refined-rule
$r_2$ in comparison to its base-rule $r_1$ as $g_{r_2,r_1}$ as:

\begin{equation}
 g_{r_2,r1}=\cfrac{conf_{r_2}}{conf_{r_1}}
\end{equation}

As we know that $r_1 \geq TS_{conf}$, then $g_{r_2,r_1}$ is upper-bounded by:

\begin{equation}
 g_{r_2,r1} \leq \cfrac{1}{TS_{conf}}
\end{equation}

More interesting for us would be base rules which satisfy the support threshold but do not satisfy the confidence
threshold, or which satisfies a quite low confidence threshold. In such case, if we have a non-uniform confidence
distribution along the buckets, it is possible that, for some specific intervals, its correspondent refined-rule
satisfies both thresholds, therefore potentially having a significant confidence gain.

In Figure ~\ref{interestingnessExample}, we illustrate that with an example where positive examples and body
support have completely different distributions and thus produce a very interesting confidence distribution. Such a
distribution is obtained thanks to the divergent rule's positives (examples that satisfy the head and body) and body
support (examples that support the body) distributions. This is shown in Figure ~\ref{interestingnessDistribution},
where we obtained these distributions by normalizing the frequency histograms
from the rule's positive examples and body support.

\begin{figure} [h!]
  \label{interestingnessExample}
  \caption{Example of frequency histograms from body support and positives support (left), and resulting confidence
distribution (right)}
  \begin{tikzpicture}[scale=0.8]
  \begin{axis}[
      ybar,
      enlargelimits=0.10,
      legend style={at={(0.5,-0.15)},
	anchor=north,legend columns=-1},
      ylabel={frequency},
      symbolic x coords={b1,b2,b3,b4,b5,b6,b7,b8,b9,b10},
      xtick=data,
      %nodes near coords,
      %nodes near coords align={vertical},
      %every node near coord/.append style={ anchor=mid west, rotate=70 },
      ]
  \addplot coordinates 
     {(b1,19) (b2,18) (b3,16) (b4,14) (b5,10) (b6,6) (b7,4) (b8,2) (b9,1) (b10,0)};
  \addplot coordinates
     {(b1,21) (b2,23) (b3,27) (b4,40) (b5,60) (b6,80) (b7,95) (b8,105) (b9,115) (b10,120)};
  \legend{postives, body support}
  \end{axis}
  \end{tikzpicture}
  \begin{tikzpicture}[scale=0.8]
  \begin{axis}[
      ybar,
      enlargelimits=0.15,
      legend style={at={(0.5,-0.15)},
	anchor=north,legend columns=-1},
      ylabel={confidence},
      yticklabel={},
      symbolic x coords={b1,b2,b3,b4,b5,b6,b7,b8,b9,b10},
      xtick=data,
      %nodes near coords,
      %nodes near coords align={vertical},
      %every node near coord/.append style={ anchor=mid west, rotate=70 },
      ]
  \addplot[fill=gray] coordinates
     {(b1,0.904761904761905)
      (b2,0.782608695652174)
      (b3,0.592592592592593)
      (b4,0.35)
      (b5,0.166666666666667)
      (b6,0.075)
      (b7,0.0421052631578947)
      (b8,0.019047619047619)
      (b9,0.00869565217391304)
      (b10,0)};
  \legend{confidence}
  \end{axis}
  \end{tikzpicture} 
\end{figure}


\begin{figure}[h!]
\begin{center}
  \label{interestingnessDistribution}
  \caption{Support distribution of body and positives from figure ~\ref{interestingnessExample}}
  \begin{tikzpicture}[scale=0.8]
  \begin{axis}[
      ybar,
      enlargelimits=0.10,
      legend style={at={(0.5,-0.15)},
	anchor=north,legend columns=-1},
      ylabel={$p(b_i)$},
      symbolic x coords={b1,b2,b3,b4,b5,b6,b7,b8,b9,b10},
      xtick=data,
      %nodes near coords,
      %nodes near coords align={vertical},
      ]
  \addplot coordinates
     {(b1,0.211111111111111)
      (b2,0.2)
      (b3,0.177777777777778)
      (b4,0.155555555555556)
      (b5,0.111111111111111)
      (b6,0.0666666666666667)
      (b7,0.0444444444444444)
      (b8,0.0222222222222222)
      (b9,0.0111111111111111)
      (b10,0)};
  \addplot coordinates
     {(b1,0.0306122448979592)
      (b2,0.0335276967930029)
      (b3,0.0393586005830904)
      (b4,0.0583090379008746)
      (b5,0.0874635568513119)
      (b6,0.116618075801749)
      (b7,0.138483965014577)
      (b8,0.153061224489796)
      (b9,0.167638483965015)
      (b10,0.174927113702624)};
  \legend{postives, body support}
  \end{axis}
  \end{tikzpicture}
  \end{center}
\end{figure}

\section{Categorical Property Definition}

In this section, we formally define a categorical property as used in the correlation lattice. 

First of all, a candidate relation must be joined with a root's non-numerical variable $X \in \mathcal{X}$ ,which will
be used for joinign with all the categorical properties.

A candidate categorical relation $r(X,Z)$, where $X$ is the join variable and $Z$ is within
a categorical domain $\mathcal{Z}$ with a finite number of categories $\{z_1,z_2,\ldots,z_n\}$, such that:

\begin{center}
$\mathcal{X}_{z_i}= \{ x | x \in \mathcal{X} \wedge \exists r(x,z_i) \}$ 
\end{center}

Typically, $|\mathcal{X}| \gg n$ and $\mathcal{X}_{z_i} \subseteq
\mathcal{X}$ with $|\mathcal{X}_{z_i}| < |\mathcal{X}|$

\begin{table}[h!]
 \label{tab:cat1}
 \caption{Example data for categorical relation $hasSex$}
 \begin{center}
  \begin{tabular}{*{2}{l}}

    hasIncome(john,20000) & hasSex(john,male) 	\\
    hasIncome(mike,30000) & hasSex(mike,male) 	\\
    hasIncome(anne,25000) & hasSex(anne,female) 	\\
    hasIncome(mary,5000)  & hasSex(mary,female) 	\\
    hasIncome(lisa,10000) & hasSex(lisa,female)	\\
    hasIncome(paul,0)	  & hasSex(paul,male)	\\
  \end{tabular}
  \end{center}
\end{table}
  
For the table ~\ref{tab:cat1}, if we define $\mathcal{X}=\{ x|\exists hasIncome(x,Y), Y \in \mathcal{Y} \}$, then we
would
have the following sub-populations of $\mathcal{X}$:

\begin{align*}
 \mathcal{X}_{male}&=\{ x|x \in \mathcal{X} \wedge \exists hasSex(x,male)\} \\
  &=\{john,mike,paul\} \\
\mathcal{X}_{female}&=\{ x|x \in \mathcal{X} \wedge \exists hasSex(x,female)\} \\
  &=\{anne,mary,lisa\}
\end{align*}


The $rdf$:$type$ property, for example, is also covered by this definition with the entity types being the categories.

A categorical property can be overlapping, i.e. with intersecting categories, or not. A non-overlapping categorical
property $r(X,Z)$, such as $hasSex$, is defined as:

\begin{center}
$\mathcal{X}_{z_i} \cap \mathcal{X}_{z_j} = \emptyset, \quad \forall z_i,z_j \in \mathcal{Z}, i \neq j$ 
\end{center}

This definition of categorical property might be too restrictive, therefore, in the next sections, we broaden this
definition by applying different techniques.

\subsection{Discretizing Numerical Properties into Categories}

Numerical properties with very large or infinite domain can also be dealt as categorical, by simply applying a
bucketing
function that maps its numerical domain into a finite set of $k$ buckets. For example, we can use a discretization
function $b$ to turn a numerical property $r(X,Z)$, with $Z \in \mathbb{R}$, into a non-overlapping categorical
property:

\begin{center}
 $b: \mathbb{R} \rightarrow \mathcal{B}$, where $\mathcal{B}=\{1,2,\dots ,k \}$
\end{center}

\begin{center}
 $r(X,b(Z)) \equiv r'(X,B) , \quad B \in \mathcal{B}$
\end{center}

This can be easily done with any discretization method, such as equal width or equal frequencies, which were
presented on Section ~\ref{sec:rw-discretization}.

\subsection{Combining Categorical Property with Linking Relation}
We can also broaden this definition by composing a categorical property with linking relations. Therewith, it is
possible to use categorical relations that do not directly join with root's join variable $X$

If a functional relation $r_1$ and a categorical $r_2$:

$r_1(X,W) \equiv f_1 : \mathcal{X} \rightarrow \mathcal{W}$ \newline
$r_2(W,Z),\quad Z \in \mathcal{Z}=\{z_1,z_2,\ldots,z_k\}$ is a categorical domain 

Then $r'(X,Z) \equiv r_1(X,W),r_2(W,Z) \equiv r_2(f_1(X),Z)$ is a composed categorical relation. 

With that, different knowledge bases can be interconnected by simply applying this definition with $owl$:$sameAs$
as linking relation. This allows to broaden the set of categorical properties to be analyzed in the correlation
lattice
to those contained in interlinked datasets.

\begin{table}[h!]
 \label{tab:cat2}
 \caption{Example of data linked to example from Table ~\ref{tab:cat1}}
 \begin{tabular}{*{2}{l}}
    owl:sameAs(john,johnSmith)& hasProfession(johnSmith,teacher) 	 \\
    owl:sameAs(mike,mikeBell) & hasProfession(mikeBell,actor) 	 \\
    owl:sameAs(anne,anneSmith)& hasProfession(anneSmith,doctor)	 \\
    owl:sameAs(lisa,lisaBell) & hasProfession(lisaBell,teacher) 
  \end{tabular}
\end{table}

Table ~\ref{tab:cat2} illustrates that, as it contains data about profession, not existent in Table ~\ref{tab:cat1}.
By combining $owl$:$sameAs$ with $hasProfession$, we can also categorize the data from the first example into
professions as follows:

\begin{align*}
\mathcal{X}_{teacher}&=\{ x|x \in \mathcal{X} \wedge \exists (owl:sameAs(x,W) \wedge hasProfession(W,teacher)\} \\
  &=\{john,lisa\} \\
\mathcal{X}_{doctor}&=\{ x|x \in \mathcal{X} \wedge \exists (owl:sameAs(x,W) \wedge hasProfession(W,actor)\} \\
  &=\{anne\}
\mathcal{X}_{actor}&=\{ x|x \in \mathcal{X} \wedge \exists (owl:sameAs(x,W) \wedge hasProfession(W,doctor)\} \\
  &=\{mike\}
\end{align*}


\subsection{Absence or Presence of a Property as a Category}

Another possibility is to define categories for the presence or absence of supporting examples for properties. With
this approach, one can also include non-categorical properties into the correlation lattice and have insight about how
the presence or absence of such property affects the distribution of a root's numerical attribute.

In this case we must consider a property which does not have examples for all its domain. For example, the property
\emph{isParentOf(x,y)} has as both domain and range type Person. If not all the Person instances of the knowledge base
have supporting facts for the relation \emph{isParentOf}, then we can split the instances in two categories: the ones
that are covered by the property and the ones that are not. For example, let \emph{hasIncome} be the root property and
let's assume we have a knowledge base with the following facts:

\begin{tabular}{*{2}{l}}
  hasIncome(john,20000) & isParentOf(john,mary) \\
  hasIncome(mike,30000) & isParentOf(mike,paul) \\
  hasIncome(anne,25000) & isParentOf(anne,paul) \\
  hasIncome(mary,5000) 	& isParentOf(lisa,mary) \\
  hasIncome(lisa,10000) & 			\\
  hasIncome(paul,0)	& 			\\
\end{tabular}

Then we could split the root node set $\mathcal{X}$ into two categories $\mathcal{X}_{parent} \subseteq \mathcal{X}$
and its complement $\mathcal{X}_{parent}^{c} \subseteq \mathcal{X}$:

\begin{align*}
 \mathcal{X}_{parent}&=\{x| x \in \mathcal{X} \land \exists isParentOf(x,Z)\} \\
  &=\{john,mike,anne,lisa\} \\
 \mathcal{X}_{parent}^{c}&=\{x| x \in \mathcal{X} \land x \not \in \mathcal{X}_{parent} \} \\
  &=\{mary,paul\} \\
\end{align*}

In this thesis we are under an open world assumption, meaning that by the absence of a given fact, we do
not assume that its negation is true. Therefore, we do not consider negated literals in the hypothesis, we
ignore the absence and just include the presence of properties in the lattice.

\subsection{Notation Used}

As we will see in the next sections, in the correlation lattice all the relations are joined by the variable of
a root's join argument $X$. So we will denote the presence of a property $a(X,Z)$ category as simply $a$, where the
variable $Z$ is free and not set to any constant. If we have a categorical property $a(X,z_i)$ with $k$ different
categories $z_i \in \mathcal{Z}=\{ z_1,z_2,\ldots,z_k\}$, we denote each of the $a(X,z_i)$ as $a_i$.

So, for example, if we set the root relation $r=hasIncome(X,Y)$ and categorical relations:
\begin{align*}
a(X,Z)&=isMarriedTo(X,Z) \\
b(X,W)&=hasSex(X,W), \quad W \in \mathcal{W} =\{male,female\} \\
c(X,V)&=bornInMonth(X,V), \quad V \in \mathcal{V} =\{january,february,\ldots,december\}
\end{align*}

With such notation, we can write large clauses in a much more concise way, such as in the examples below:
\begin{align*}
rab_1c_3 &\equiv hasIncome(X,Y),isMarriedTo(X,Z),hasSex(X,male),bornInMonth(march) \\
rab_2c_{11} &\equiv hasIncome(X,Y),isMarriedTo(X,Z),hasSex(X,female),bornInMonth(november)
\end{align*}

\section{Interestingness Measure}

As discussed before, we are interested in rules that have divergent positives and body support distributions, or in
other words, adding the head to the body clause should result in a different distribution along the target numerical
attribute $Y$. Therefore, we define the interestingness of adding a literal $l$ to a clause $c$ as the
dissimilarity of $c \wedge l$ and $c$ support distributionw along $Y$.

It is possible to use any state-of-the-art dissimilarity measure, such as the ones presented
in ~\ref{sec:infotheoreticmeasures}. Kullback-Leibler or its metrified version Jensen-Shannon are good options, and
can be directly applied on normalized frequency histograms. However, using a divergence
measure alone as interestingness measure is maybe cause some problems.

Firstly, it is important to take into account the sampling error. The lower the support of a clause, the higher the
expected fluctuations of the observed distribution in relation to the sampling distribution. That means that if we
draw two distributions from the same sampling distribution, one with more examples than the other, and measure the
divergence between each of them and the sampling distribution, the smaller sample is expected to have a higher
divergence.

Because of that, divergence measure alone tends to rank clauses with smaller support as more interesting. In order to
compansate such effect, we can combine a divergence measure with the support, by simply multiplying their values.
Morover, including support into our interestingness measure is important because, after all, we are still interested
in rules with high support.

\section{Preprocessing}

In this section, we will present the preprocessing steps required by our proposed algorithm. It basically consists of
first building a joinable relations map for each join pattern, according
to relations domain and range types as well as support threshold. Afterwards, we search the available categorical
properties for each numerical relation that will be used in the correlation lattice. At last we build the lattice
itself, which belongs to the preprocessing step but will be discussed in the next section.

\subsection{Relation Preprocessing}

In this step, we focus on creating a map of joinable for each join pattern between two relations. As in this thesis we
are working with RDF triples, we have four possible join patterns:

\begin{itemize}
 \item Argument 1 on Argument 1: e.g. \emph{hasIncome(\textbf{X},Y)hasAge(\textbf{X},Z)}
 \item Argument 1 on Argument 2: e.g. \emph{hasIncome(\textbf{X},Y)isMarriedTo(Z,\textbf{X})}
 \item Argument 2 on Argument 1: e.g. \emph{livesIn(Y,\textbf{X})isLocatedIn(\textbf{X},Z)}
 \item Argument 2 on Argument 2: e.g. \emph{livesIn(Y,\textbf{X})wasBornIn(Z,\textbf{X})}
\end{itemize}

With that we could easily obtain all the possible pair of joinable relations for each join pattern, avoiding testing
hypothesis containing invalid join pairs.

\subsubsection{Exploiting Relation Range and Domain Types}

A knowledge base is expected to have an ontology defining the structure of the stored data (the types of entities and
their relationships). Additionally, every relation's range (type of \ord{1} argument) and domain (type of \ord{2}
argument) should be defined. These information can help us identify the allowed joining relations for each join
pattern.

\begin{figure}[h!]
  \label{fig:hierarchy}
  \caption{Type hierarchy example}
  \centering
  \begin{tikzpicture}[<-,draw=black,very thick,level/.style={sibling distance = 6cm/#1,level distance = 1.5cm}]
    
    \node{Thing}
      child{node {Location} 
	child{node {Country}}
	child{node {City}}
      }
      child{node {Person}
	child{node {Athlete}
	  child{node {Marathoner}}
	  child{node {Swimmer}}
	}
	child{node {Scientist}}
	child{node {Artist}
	  child{node {Painter}}
	  child{node {Musician}}
	  child{node {Poet}}
	}
      };
  \end{tikzpicture}

\label{fig:typeHierarchy}
\end{figure}

Assuming that the knowledge base has its type hierarchy described with the relation \emph{rdfs:subClassOf}, such as
the
example shown in Figure ~\ref{fig:hierarchy}, where each arrow means that the tail is a subclass of the head. Also,
we assume that every property has both argument types declared with \emph{rdfs:domain} and \emph{rdfs:range}. With
such information, it is a straightforward task to prune out the pairs of joining relations which are not allowed by
the type hierarchy.    

For every possible pair of relations, we simply try to match the joining argument types from the 2 joining relations.
We
check whether they are equal or if one can be subsumed by the other, i.e., one is a subclass of the other. If so, then
it is allowed to join the pair of relations on the given joining arguments, or in other words, the type hierarchy does
not prevent them from joining. The pseudo-code for performing such task is shown in the Algorithm ~\ref{alg1}:

\begin{algorithm}[h!]
  \caption{Function $checkTypes$ \newline Checks whether two relations are joinable for a given join pattern}
 \label{alg1}
 \SetKwFunction{subsumes}
 \KwIn{\textbf{Input:} $r_i$, $r_j$: Joining relations, $arg_i$, $arg_j$: Joining arguments \\}
 \KwOut{True if $arg_i$ from $r_i$ joins with $arg_j$ from $r_j$, False otherwise}
  \Switch{$arg_i$} {
      \Case{$1$}{
	$type_i \leftarrow r_i.domain$ \;
      }
      \Case{$2$}{
	$type_i \leftarrow r_i.range$ \;
      }
  }
  \Switch{$arg_j$} {
      \Case{$1$}{
	$type_j \leftarrow r_j.domain$ \;
      }
      \Case{$2$}{
	$type_j \leftarrow r_j.range$ \;
      }
  }
  \eIf{$type_i = type_j$ {\bf or} \FuncSty{isSubClassOf(}$type_i$,$type_j$\FuncSty{)} {\bf or}
       \FuncSty{isSubClassOf(}$type_j$,$type_i$\FuncSty{)}}{
      \Return true\;
   }{
    \Return false\;
  }
\end{algorithm}

Nevertheless, it might be that in the knowledge base, the cardinality of such join might be zero or simply not exceed
the support threshold. Thus, it is worth to that beforehand, and that is what will be explained in the next section.

\subsubsection{Exploiting Support Monotonicity}

Support is a monotonically decreasing measure in top-down ILP. So we know that by adding any
literals to the hypothesis, we can only get a smaller or equal support. Therefore, for each pair of joinable relations
in each of the join patterns, we can query the knowledge base and check whether they have enough supporting facts.

Thus, if any pair of relations does not reach the minimum support for a given join pattern, we know that any clause
containing such join will therefore fail the support test as well, so we do not need to test such a hypothesis in the
core ILP algorithm.

\begin{algorithm}[h!]
  \caption{Function $checkSupport$ \newline Checks whether join support exceeds threshold}
  \label{alg2}
  \SetKwFunction{executeQuery}
  \KwIn{\textbf{Input:} $r_i$, $r_j$: Joining relations, $arg_i$, $arg_j$: Joining arguments, $supportThreshold$:
Support threshold \\ }
  \KwOut{True if join support exceeds threshold, False otherwise}
    \Switch{$(arg_i,arg_j)$} {
      \Case{$(1,1)$}{
	$query \leftarrow$ \emph{``select count distinct $?x$ where \{ $?x$ <$r_i$> $?y$ . $?x$ <$r_j$> $?z$ \}''} \;
      }
      \Case{$(1,2)$}{
	$query \leftarrow$ \emph{``select count distinct $?x$ where \{ $?x$ <$r_i$> $?y$ . $?z$ <$r_j$> $?x$ \}''} \;
      }
      \Case{$(2,1)$}{
	$query \leftarrow$ \emph{``select count distinct $?x$ where \{ $?y$ <$r_i$> $?x$ . $?x$ <$r_j$> $?z$ \}''} \;
      }
      \Case{$(2,2)$}{
	$query \leftarrow$ \emph{``select count distinct $?x$ where \{ $?y$ <$r_i$> $?x$ . $?z$ <$r_j$> $?x$ \}''} \;
      }
    }
    $joinSupport \leftarrow$ executeQuery($query$)\;
     \eIf{$joinSupport \ge supportThreshold$} {
      \Return true\;
    }{
      \Return false\;
    }
\end{algorithm}


The preprocessing is done by Algorithm ~\ref{alg3}, which applies ~\ref{alg1} and ~\ref{alg2} on all the possible join
pair combinations and extracting the valid ones, we can build 4 joining maps, one for each join pattern. Each map has
relations as keys and a set of joinable relations as value. In the refinement step at the ILP algorithm, these maps
will
be queried in order to obtain suggestions of literals to be
added.

\begin{algorithm}[h!]
  \caption{Preprocessing algorithm}
  \label{alg3}
  \SetKwFunction{executeQuery}
  \KwIn{\textbf{Input:} $r$: Set of Relations, $supportThreshold$: The support threshold \\ }
  \KwResult{$L_{m,n}(r_i)$: List of joinable relations, where $m,n \in \{1,2\}, r_i \in r$}

    \tcp{Lists initialization}
    \ForEach{$r_i \in r$} {
      $L_{1,1}(r_i) \leftarrow r_i$ \;
      $L_{2,2}(r_i) \leftarrow r_i$ \;
      $L_{1,2}(r_i) \leftarrow \emptyset$ \;
      $L_{2,1}(r_i) \leftarrow \emptyset$ \;
    }
    \tcp{For every possible pair of relations}
    \ForEach{$r_i \in r$} {
	\ForEach{$r_j \in r$ {\bf and} $j \geq i$} {
	    \If{$r_i \neq r_j$} {
		\If{$checkTypes(r_i,r_j,1,1)$ {\bf and} $checkSupport(r_i,r_j,1,1)$} {
		    $L_{1,1}(r_i) \leftarrow L_{1,1}(r_i) \cup r_j$ \;
		    $L_{1,1}(r_j) \leftarrow L_{1,1}(r_j) \cup r_i$ \;
		}
		\If{$checkTypes(r_i,r_j,2,2)$ {\bf and} $checkSupport(r_i,r_j,2,2)$} {
		    $L_{2,2}(r_i) \leftarrow L_{2,2}(r_i) \cup r_j$ \;
		    $L_{2,2}(r_j) \leftarrow L_{2,2}(r_j) \cup r_i$ \;
		}
	    }
	    \If{$checkTypes(r_i,r_j,1,2)$ {\bf and} $checkSupport(r_i,r_j,1,2)$} {
		$L_{1,2}(r_i) \leftarrow L_{1,2}(r_i) \cup r_j$ \;
		$L_{2,1}(r_j) \leftarrow L_{2,1}(r_j) \cup r_i$ \;
	    }
	    \If{$checkTypes(r_i,r_j,2,1)$ {\bf and} $checkSupport(r_i,r_j,2,1)$} {
		$L_{2,1}(r_i) \leftarrow L_{2,1}(r_i) \cup r_j$ \;
		$L_{1,2}(r_j) \leftarrow L_{1,2}(r_j) \cup r_i$ \;
	    }
	}
   }
\end{algorithm}

\subsection{}

\section{Correlation Lattice}
\label{ch:lattice}

The idea is to build during preprocessing a graph inspired in the itemset lattice that describes the influence of
different categorical relations on a given numerical attribute's distribution. We call such graph a correlation
lattice.
Comparing to a Itemset Lattice, in a correlation lattice we have a set of categorical relations (that are joined with
root property) instead of items. In addition, Each node in the graph has an associated histogram with the support
distribution over the root's numerical attribute.

To illustrate the idea, let's analyze a simple real-world example with the $hasIncome(X,Y)$ relation. If we have two
categorical relations, one strongly correlated to income, e.g. $hasEducation$, and one uncorrelated (or very weakly
correlated), e.g. $wasBornInMonth$.

Let's assume that for the relation $wasBornInMonth(X,Z)$ we have the 12 months from the Gregorian Calendar as
constants and for $hasEducation(X,Z)$ we can have 10 different categorical constants for $Z$: ``Preschool'',
``Kindergarten'', ``ElementarySchool'', ``MiddleSchool'', ``Highschool'', ``Professional School'', ``Associate's
degree'', ``Bachelor's degree'', ``Master's degree'' and ``Doctorate degree''. 

It's expected that the income distribution will be roughly the same for people born in any of the months, whereas
for different education levels, e.g. Elementary School and Doctoral Degree, their income distribution are expected to
be
different from each other them and different from the overall income distribution.

Based on this idea, we basically check how different categorical relations affect a numerical distribution. Such
information, together with other measures such as support, provides valuable cues on what categorical attributes and
what
categorical constants might be the most interesting to be added to the hypothesis in the core ILP algorithm.

\subsection{Building the Lattice}

We first start with the chosen numerical property, for example $hasIncome(X,Y)$, which should have at least two
arguments, one being a joining variable $X$ and the other the numerical attribute variable $Y$. Firstly, we query the
examples distribution along the numerical attribute $Y$, arbitrarily choose a number of buckets $k$ and discretize
its $Y$'s domain into buckets $b_1, b_2, \dots, b_k$ using any unsupervised discretization method. 

We then build a frequency histogram of the root node with the its overall population. We define the histogram of a
node
$n$ with numerical attribute variable $Y$ as a $k$-dimensional vector $h(n)$ where:

\begin{equation}
 h(n)=<h_1(n),h_2(n),\ldots,h_k(n)>
\end{equation}

where $h_i(n)=supp(n|Y \in b_i)$, and therefore:

\begin{equation}
  |h(n)|_1=\sum_{i=1}^{k}h_i(n)=supp(n)
\end{equation}

Every node in the lattice is composed by conjunctive clause of non-negated literals and contains a frequency histogram
of its population. For the sake of comparison between, all histograms are built on the same buckets defined in the
root
node. 

Subsequently, for the chosen set of categorical relations, we do the same with each subpopulation of examples from
each
category obtained by joining the root with its corresponding categorical literal. For every category a node in
the lattice is created with associated frequency histogram. In addition, the edges of the lattice are created by
setting the root as parent node, and adding the new nodes as children from the root.

In a further step, we try to join every possible pair of categorical relations and including the constants. For the
given, example with the relations \emph{hasEducation} and \emph{wasBornInMonth} we would then create the nodes:

  \emph{hasIncome(X,Y)wasBornInMonth(x,``January''),hasEducation(x,``Preschool'')} \newline
  \emph{hasIncome(X,Y)wasBornInMonth(x,``January''),hasEducation(x,``Kindergarten'')} \newline
  \dots \newline
  \emph{hasIncome(X,Y)wasBornInMonth(x,``January''),hasEducation(x,``Doctorate Degree'')} \newline

  \emph{hasIncome(X,Y)wasBornInMonth(x,``February''),hasEducation(x,``Preschool'')} \newline
  \emph{hasIncome(X,Y)wasBornInMonth(x,``February''),hasEducation(x,``Kindergarten'')} \newline
  \dots \newline
  \emph{hasIncome(X,Y)wasBornInMonth(x,``February''),hasEducation(x,``Doctorate Degree'')} \newline
 
  \dots \newline

  \emph{hasIncome(X,Y)wasBornInMonth(x,``December''),hasEducation(x,``Preschool'')} \newline
  \emph{hasIncome(X,Y)wasBornInMonth(x,``December''),hasEducation(x,``Kindergarten'')} \newline
  \dots \newline
  \emph{hasIncome(X,Y)wasBornInMonth(x,``December''),hasEducation(x,``Doctorate Degree'')} \newline


 \begin{figure}[!h]
  \caption{Combination of relations $hasEducation$ and $bornInMonth$ }
  \centering
  \begin{tikzpicture}[scale=1.75,auto=center,every node/.style={,font=\tiny}]
    \node (r) 	at (5,5)  {$hasIncome(X,Y)$};
    \node[text width=1cm,align=center] (a1) 	at (1,3) {$hasIncome(X,Y),$\\$bornInMonth(X,january)$};
    \node[text width=1cm,align=center] (an)	at (3,3) {$hasIncome(X,Y),$\\$bornInMonth(X,december)$};
    \node[text width=1cm,align=center] (b1)	at (7,3) {$hasIncome(X,Y),$\\$hasEducation(X,preschool)$};
    \node[text width=1cm,align=center] (bm)	at (9,3) {$hasIncome(X,Y),$\\$hasEducation(X,phd)$};
    \node[font=\normalfont] (da)	at (3.5,4) {$\dots$};
    \node[font=\normalfont] (db)	at (6.5,4) {$\dots$};
    \node[font=\normalfont] (dn)	at (9,4) {$\dots$};
    \node[font=\normalfont] (d1)	at (2.9,1) {$\dots$};
    \node[font=\normalfont] (d2)	at (5.4,1) {$\dots$};
    \node[font=\normalfont] (d3)	at (7.9,1) {$\dots$};

    \node[text width=1cm,align=center] (a1b1) 	at (1.0,1)
{$hasIncome(X,Y),$\\$bornInMonth(X,january),$\\$hasEducation(X,preschool)$};
    \node[text width=1cm,align=center] (a1bm)	at (3.5,1)
{$hasIncome(X,Y),$\\$bornInMonth(X,january),$\\$hasEducation(X,phd)$};
    \node[text width=1cm,align=center] (anb1) 	at (6.0,1)
{$hasIncome(X,Y),$\\$bornInMonth(X,december),$\\$hasEducation(X,preschool)$};
    \node[text width=1cm,align=center] (anbm)	at (8.5,1)
{$hasIncome(X,Y),$\\$bornInMonth(X,december),$\\$hasEducation(X,phd)$};
    \foreach \from/\to in
      {r/a1,r/an,r/b1,r/bm,a1b1/a1,a1b1/b1,a1bm/a1,a1bm/bm,anb1/an,anb1/b1,anbm/an,anbm/bm}  
    \draw (\from) -- (\to);
  \end{tikzpicture}
  \label{fig:combiningLiterals}
\end{figure}

This process works just like in an itemset lattice until a predetermined maximum level or until all the
possible combinations are exhausted. Figure ~\ref{fig:lattice} shows how a correlation lattice looks like.

\begin{figure}[!h]
  \caption{Correlation Lattice example}
  \centering
  \begin{tikzpicture}
  [scale=1.8,auto=center,every node/.style={draw=black, font=\tiny}]
  \input{./Chapters/graph1}
  \end{tikzpicture}
  \label{fig:lattice}
\end{figure}

\subsection{Pruning Opportunities}

In this section we discuss about safe pruning opportunities that can be explored whilst building the correlation
lattice: support and conditional independence. As these might not be sufficient to reduce lattice size to a feasible
level, later, in section ~\ref{sec:heuristics}, we will also discuss possible pruning techniques.

\subsubsection{Support}

As described in (\cite{LavracDz94}), in top-down ILP every refinement causes the support to decrease, therefore we
know
that for every node in the correlation lattice, its support will be greater or equal than any of its children, so
support is a monotonically decreasing measure so we can safely prune a node that does not reach the minimum support
threshold. Therefore, we can use the apriori-style pruning, just like in the core-ILP and in the association rule
mining.

In this thesis, we define support as the absolute frequency of supporting facts. So the support of a node $n$ is
defined as:

$supp(n)=|{x|x \in n}|$


\subsubsection{Independence Checks}

By simplicity, we assume that every possible pair of categorical relations are independent given their common parent
and
we search for evidence to prove the contrary.

For 2 nodes to be joined, they must have a common parent, i.e. two nodes at level $l$ (with $l+1$ literals) are
joinable
if they share $l$ literals. Therefore, it is straightforward to calculate the conditional probabilities of each of the
joining nodes given the common parent, and estimate the frequency distribution for the conditional independence case.

Let's say we have the following join case:
 
\begin{figure}[!h]
  \caption{Node join example for independence test}
  \centering
  \begin{tikzpicture}
  [scale=1,auto=center,every node/.style={minimum size=1cm}]
    \node (p)  [circle,fill=black!20] at (4,10) {$n$};
    \node (n1) [circle,fill=black!20] at (3,8)  {$nx$};
    \node (n2) [circle,fill=black!20] at (5,8)  {$ny$};
    \node (n12)[circle,fill=black!20] at (4,6)  {$nxy$};

    \foreach \from/\to in {p/n1,p/n2,n1/n12,n2/n12}
      \draw (\from) -- (\to);

    \draw[dashed] (2,9) -- (8,9);
    \draw[dashed] (2,7) -- (8,7);

    \node (level0)[font=\small] at (7,10) {level $l-1$};
    \node (level1)[font=\small] at (7,8)  {level $l$};
    \node (level2)[font=\small] at (7,6)  {level $l+1$};
  \end{tikzpicture}
  \label{fig:joinIndepExample}
\end{figure}

In the example shown in figure ~\ref{fig:joinIndepExample}, $x$ and $y$ are literals and $n$ is a node with clause
$c_n$,
such that $x \neq y$ and $x,y \not \in c_n$. The nodes $nx$ and $ny$ are formed by adding $x$ and $y$ to $n$, so
$c_{nx}=\{c_n,x\}$ and $c_{ny}=\{c_n,y\}$. With the histogram from these three nodes, we can calculate the conditional
probability $p_i(x|n)$ and $p_i(y|n)$, then calculate $\hat{h_i}(nxy)$, which is the estimation of
$h_i(nxy)$ assuming that $x$ and $y$ are independent given common parent $n$.

\begin{align*}
\label{eq:condindep}
 p_i(x|ny) &= p_i(x|n) \\ 
 &= \cfrac{h_i(nx)}{h_i(n)} \\ 
 p_i(y|nx) &= p_i(y|p) \\ 
 &= \cfrac{h_i(ny)}{h_i(n)} \\ \\ 
 \hat{h_i}(nxy) &= p_i(x|ny)p_i(y|n)*h_i(n) \\ 
 &= p_i(x|p)h_i(yp) \\ 
 \hat{h_i}(nxy) &= p_i(y|nx)p_i(x|n)*h_i(n) \\ 
 &= p_i(y|n)h_i(xn)
\end{align*}

After that, we query the actual frequency distribution on the knowledge base and do a Pearson's chi-squared
independence test. As null hypothesis and alternative hypothesis we have:

\begin{itemize}
 \item $H_0$ = \emph{$x$ and $y$ are conditionally independent given their common parent $p$}
 \item $H_1$ = \emph{$x$ and $y$ are conditionally dependent given their common parent $p$} 
\end{itemize}

Number of degrees of freedom is the number of buckets minus one:

\begin{center}
 $df=k-1$
\end{center}

We calculate the critical value $\chi^2$:

\begin{equation}
 \chi^2=\sum_{i=1}^{k} \cfrac{(h_i - \hat{h_i})^2}{\hat{h_i}}
\end{equation}

\cite{Jaroszewicz02pruningredundant}

Then it is possible to obtain the p-value and check whether there is enough confidence to reject the null hypothesis
$H_0$. 


In other words if we find out that $x$ and $y$ are conditionally independent given $p$, we could rewrite the equation
~\ref{eq:condindep} as the following rules being equivalents, with similar accuracy distributions:

\begin{center}
  $x \leftarrow py \equiv x \leftarrow p \quad$ and  $\quad y \leftarrow px \equiv y \leftarrow p $
\end{center}

Therefore, we know that if we have $x$ fixed as head of clauses in the core ILP, and we currently have $x \leftarrow
p$
joining the node $px$ with $py$ to obtain the rule $x \leftarrow py$ does not add any valuable information. The same
applies
for having $y$, and obtaining $y \leftarrow px$ from $y \leftarrow p$ by joining the same pair of nodes. This property
plays an important role in the integration of the correlation lattice into the core ILP as we will explain in more
details later in Section (???).

Nevertheless it cannot be safely pruned from the lattice.  

\begin{comment}
In level 1 from \graphname, nodes can be directly pruned, on the other hand, for further levels, for a node to be
pruned by independence, all the possible joins resulting the node must be independent. In level 2, for example, in
order
to prune the node $r a_1 b_1 c_1$, given that in level 1 the nodes $r a_1 b_1$, $r a_1 c_1$ and $r b_1 c_1$ were not
pruned. All the three possible join combinations should fail the independence test, i.e.:

\begin{equation}
\begin{split} 
  freq(r a_1 b_1 c_1) &\approx freq(r a_1)p (r b_1|r a_1) p(r c_1|r a_1) \\ 
  &\approx  freq(r b_1) p(r a_1|r b_1) p(r c_1|r b_1) \\ 
  &\approx  freq(r c_1) p(r a_1|r c_1) p(r b_1|r c_1)  
\end{split}
\end{equation}
\end{comment}

This applies to nodes at any level $l$, with $p \leq l$ parents and $C_{2}^{p}$ possible join pairs. If any of the
join
pairs has enough evidence of being dependent, then 2 edges are created connecting each of the joined
nodes to the result of their join

\subsection{Entropy Divergence Measures}

As seen in the previous sections, we are interested in rules whose base-rule has accuracy below threshold, but
contains
one or multiple specific intervals with accuracy above threshold. For this to happen, we need a rule with non-uniform
accuracy distribution, or in other words, divergent body support and rule positive examples distributions.

Therefore, we are interested in adding categories that produces distributions different from their parent nodes'. In
order to measure such divergence between distributions, some of the state-of-the-art such as the following ones can be
used.

\begin{itemize}
 \item Kullback-Leibler \cite{Kullback51klDivergence}: 
    \begin{equation}
      D_{KL}(P||Q)=\sum_{\substack{i}}\ln\left(\cfrac{P(i)}{Q(i)}\right)*P(i)
    \end{equation}
 \item Chi-squared ($\chi^2$):
    \begin{equation}
      D_{\chi^2}(P||Q)=\sum_{\substack{i}}\cfrac{(P(i)-Q(i))^2}{P(i)}
    \end{equation}
 \item Jensen-Shannon \cite{17795}:
    \begin{equation}
      D_{JS}(P||Q)=\cfrac{1}{2}D_{KL}(P||M)+\cfrac{1}{2}D_{KL}(Q||M), 
    \end{equation}
\end{itemize}

Where $P$ and $Q$ are discrete distributions to be compared and $M=\cfrac{1}{2}(P+Q)$. In the lattice context, these
distributions would be the frequency histogram normalized to 1, and nodes directly connected by edges would have their
distributions compared.

As discussed in \cite{17795}, although Jensen-Shannon is computationally more expensive, it has the advantage of being
a
symmetric and smoothed version of the Kullback-Leibler measure.

These divergence measures are important for identifying potential accuracy distributions. If the divergence between
the
frequency distributions of a parent and child node is big, that means that if we have a rule with the parent node as
body and the additional literal from the child as head, its accuracy distribution will be not uniform and therefore
potentially interesting for searching for refined-rules with numerical intervals.


\subsection{Heuristics}
\label{sec:heuristics}

As seen before, the number of nodes in a correlation lattice grows exponentially with the number of categorical
relations and its constants. For $n$ categorical relations, each with $m$ constants, the total number of nodes is
$2^{nm}$. As we limit the number of levels in the lattice to $l$, the total number of nodes reduces to:

\begin{center}
  \begin{equation}
    \sum_{i=1}^{l}\binom{nm}{i}
  \end{equation}
\end{center}

if $l=nm$:

\begin{center}
  \begin{equation}
    \sum_{i=1}^{mn}\binom{nm}{i} = 2^{nm}
  \end{equation}
\end{center}


Pruning by support is usually not sufficient to make it feasible and it is necessary to apply
heuristics to prune it more aggressively.

%%% Divergence

Pruning by divergence is clearly not safe. Let's suppose we have a root numerical property $r(x,y)$, and two
categorical
relations $a(x,z)$ with constants $A_1$ and $A_2$ and $b(x,w)$ with constants $B_1$ and $B_2$. For simplicity, let's
assume y is divided in two buckets and root $r$ has an uniform distribution $[0.5 \, 0.5]$ from frequencies $[2 \,
2]$.
It is possible to have $r a_1$ and $r a_2$ as well as $r b_1$ and $r b_2$ with the same uniform distribution with
frequencies $[1 \, 1]$. Nevertheless when combined we can have the following:

$r a_1 b_1 : [1.0 \, 0.0] \\$
$r a_1 b_2 : [0.0 \, 1.0] \\$
$r a_2 b_1 : [0.0 \, 1.0] \\$
$r a_2 b_2 : [1.0 \, 0.0]$

As shown above, divergence is not a monotonically decreasing measure, thus it can only be used as heuristics.

% Draw example instead to explain better

Moreover, using a divergence measure alone might also be problematic. Histograms with low support are more likely to
present a higher divergence than histograms with higher support, supposing that they were drawn from the same original
distribution. Consequently, and algorithm using only the divergence as heuristics would end up giving preference to
nodes with lower support. Therefore, it is important to use divergence combined with support as pruning heuristics.
[Restructure, it's confusing]

%Moreover, we are interested not only in rules with high confidence, but also rules with high support so more facts
can
%be derived.

% Talk about how to use such measure? (Threshold, Top-k, Cost-Benefit)

%%% Independence

As seen seen in Section (???), checking for conditional independence of categories is very insightful and
can detect equivalent rules like in Figure ~\ref{fig:joinIndepExample} where we know that 
$x \leftarrow py \equiv x \leftarrow p$ and $y \leftarrow px \equiv y \leftarrow p$. Nevertheless, we cannot prune the
node $pxy$ from the lattice as we $x$ and $y$ might not be independent given $pz$, and for instance, a rule $x
\leftarrow pyz$ might not be equivalent to $x \leftarrow py$.

\section{Bucketing the Numerical Attributes}

So far we have mentioned that the root's numerical attribute domain should be discretized by dividing it in buckets in
order to build frequency histograms and compare distributions. In this section, we will discuss about how to perform
such discretization.

The buckets used throughout the whole correlation lattice should be consistent, and the bucketing method as well as
the
boundaries are specified in the very beginning with the root node. It should have as input an arbitrarily defined
number
of buckets $k$.

Since we don't consider any labels for the examples, we use one of the unsupervised discretization methods presented
in
section ~\ref{sec:discretization}: equal frequencies or equal width. We choose the method according to the domain
characteristics and define the bucket boudaries based on the overall population distrution.

\section{Incorporating Correlation Lattice into the Core ILP Algorithm}

The ILP core learning algorithm requires some modifications in order to support the correlation lattice. As stated
before, we use the top-down search, starting from the most general clauses then further refining them by introducing
new
substitutions or literals until stopping criteria is reached.

[...]

\subsection{Querying the Correlation Lattice}

In the ILP there is a fixed head literal and such literal can and should be included in the correlation lattice. In
case
it is present in the lattice, it plays a key role in the search. Once the root property is added in a clause that does
not exceed the accuracy threshold.

In such case we are interested in searching the most interesting literals to add. First step would be to search in the
lattice the literals already present in both head and body of the clause that are joined with the root. Nevertheless,
what we are not really interested in the benefit of adding a literal to the conjunction of rule's head and body, but
in
the benefit of adding the head to a body with a new literal.

For example, if we have the clause $a$:-$r$, $r$ is the root of a correlation lattice and the head $a$ is included in
it. After searching in the graph for the literals joined with root present in the clause we would come to node
$ra$. We are not searching for the most interesting child from $ra$, but for the children, which has a parent without
the head literal that has the has the highest interestingness measure when adding the head.

\begin{figure}[!h]
  \caption{Interestingness of adding a literal to the body of a clause}
  \centering
  \begin{tikzpicture}
  [scale=1,auto=center,every node/.style={minimum size=1cm}]
    \node (r)  [circle,fill=black!20] at (4,10) {$r$};
    \node (ra) [circle,fill=black!20] at (1,8)  {\textbf{\emph{r,a}}};
    \node (rb) [circle,fill=black!20] at (3,8)  {$r,b$};
    \node (rc) [circle,fill=black!20] at (5,8)  {$r,c$};
    \node (rd) [circle,fill=black!20] at (7,8)  {$r,d$};
    \node (rab)[circle,fill=black!20] at (3,5)  {$r,a,b$};
    \node (rac)[circle,fill=black!20] at (5,5)  {$r,a,c$};
    \node (rad)[circle,fill=black!20] at (7,5)  {$r,a,d$};

    \foreach \from/\to in {r/ra,r/rb,r/rc,r/rd,ra/rab,ra/rac,ra/rad,rb/rab,rc/rac,rd/rad}
      \draw[black!70] (\from) -- (\to);

    \node (m1)[blue] at (3,7)  {\textbf{$m_{a \leftarrow r,b}$}};
    \node (m2)[blue] at (5,7)  {$m_{a \leftarrow r,c}$};
    \node (m3)[blue] at (7,7)  {$m_{a \leftarrow r,d}$};
    \node (m4)[red] at (2.3,6)  {$m_{b \leftarrow r,a}$};
    \node (m5)[red] at (3.6,6)  {$m_{c \leftarrow r,a}$};
    \node (m6)[red] at (5.0,6)  {$m_{d \leftarrow r,a}$};

    %\draw (ra) -- (rab) -|  \node [below,pos=0.25] {$m_1$}(l1);
  \end{tikzpicture}
  \label{fig:latticeSuggestion}
\end{figure}

As shown in Figure ~\ref{fig:latticeSuggestion}, if we can add $b$, $c$ or $d$ to $ra$, we are interested about the
measures in blue instead of the ones in red. Nevertheless, searching for values for $m_{a,rb}$, $m_{a,rc}$ and
$m_{a,rd}$ is a bit tricky. It is necessary to check for all children of $ra$, which ones have parents without head
literal $a$, then gather all the measures and rank them.

In order to make it simpler, we can create a suggestions map during lattice build. This map would contain an entry for
all node it is joined to, with head as key and literals to add sorted by measure as value. So for example when we are
obtaining the node $rab$ by joining $ra$ and $rb$, we add to $ra$ an entry with $a$ as key, $b$ as new literal and
associated measure $m_{a,rb}$ and we also add to $rb$ an entry with $b$ as key, $a$ as new literal ad associated
measure
$m_{b,ra}$. This is shown more clearly in algorithm ~\ref{alg:buildmaps}.

\begin{algorithm}[h!]
 \caption{Suggestion map build when joining nodes during lattice build}
 \label{alg:buildmaps}
 \KwIn{\textbf{Input:} $nx, ny$: Joining nodes, $n$: Common parent node}
  $\hat{h}(nxy) \leftarrow h(nx)h(ny)/h(n)$ \;
  $h(nxy) \leftarrow $\FuncSty{queryFrequencyHistogram(}$nxy$\FuncSty{)} \;
  $\chi^2 \leftarrow \sum_{i=1}^{k}(h_i(nxy)-\hat{h}_i(nxy))^2/(\hat{h}_i(nxy))$ \;
  $dof \leftarrow k-1$ \;
  $pValue = $\FuncSty{criticalValue(}$\chi^2,dof$\FuncSty{)} \;
  \If{$pValue \geq minPValue$}{
    $m_{x,ny} \leftarrow $\FuncSty{interestingness(}$h(nxy),h(ny)$\FuncSty{)} \;
    $m_{y,nx} \leftarrow $\FuncSty{interestingness(}$h(nxy),h(nx)$\FuncSty{)} \;
    $nx.suggestionsMap(x) \leftarrow (y,m_{x,ny})$ \;
    $ny.suggestionsMap(y) \leftarrow (x,m_{y,nx})$ \;
  }
\end{algorithm}

In the example of Figure ~\ref{fig:latticeSuggestion}, $ra$ would contain the following map:

\begin{center}
  \begin{tabular}{r | l}
    a & b [$m_{a,rb}$] \\
      & c [$m_{a,rc}$] \\
      & d [$m_{a,rd}$]
  \end{tabular}
\end{center}

As in this example the node $ra$ has only two literals, and the root $r$ cannot be the head, we use the node $ra_1b_1$
in Figure ~\ref{fig:lattice} to better illustrate it:

\begin{center}
  \begin{tabular}{r | l}
    a1 	& c1 [$m_{a_1,rb_1c_1}$] \\
	& c2 [$m_{a_1,rb_1c_2}$] \\
    \hline
    b1	& c1 [$m_{b_1,ra_1c_1}$] \\
	& c2 [$m_{b_1,ra_1c_2}$]
  \end{tabular}
\end{center}

In a node at level $l$ with $l+1$ literals (one being the root), we can have up to $l$ keys in the map. Therewith, we
can much more easily find the best suggestions for any possible head literal without having to visit other nodes.

First we need to introduce a new refinement operator that extracts the support and accuracy distributions along a chosen
numerical property and searches for interesting ranges. With that there are

\begin{algorithm}[!h]
 \caption{Refinement step}
 \label{alg4}
 \SetKwFunction{checkNumericalRanges}
 \KwIn{\textbf{Input:} $clause$: Set of literals from the clause, $lattice(r_{root})$: Set of existent Correlation Lattices, 
 $lit_{body}$: Literal in clause that will be joined with new Literal, $lit_{new}$: Literal to be added to the clause,
 $arg_{body}$: Join argument from body literal, $arg_{new}$: Join argument from new literal \\}
 \KwOut{True if $arg_i$ from $r_i$ joins with $arg_j$ from $r_j$, False otherwise}
 
  $r_{new} \leftarrow lit_{new}.relation$ \;
  \If{$arg_{new} = 1$ \textbf{and} $r_{new}$ is numerical \textbf{and} $\exists$ $lattice(r_{new})$} {
    $node \leftarrow$ $lattice(r_{new}).root$.\FuncSty{search(}$head$\FuncSty{)} \;
    \ForEach{$lit_i \in clause.body$} {
      $node \leftarrow node$.\FuncSty{search(}$lit_i$\FuncSty{)} \;
    }
    \FuncSty{checkNumericalRanges(}$clause$,$lit_{new}$,$node$\FuncSty{)} \;
  }
\end{algorithm}

\begin{algorithm}[!h]
 \caption{checkNumericalRanges}
 \label{alg4}
 \SetKwFunction{queryAccuracyDistribution, querySupportDistribution}
 \KwIn{\textbf{Input:}  \\}
 \KwOut{}
  $v \leftarrow $ Numerical variable from $lit_{new}$ \;
  $acc \leftarrow $ \FuncSty{queryAccuracyDistribution(}$clause$,$v$\FuncSty{)} \;
  $sup \leftarrow $ \FuncSty{querySupportDistribution(}$clause$, $v$\FuncSty{)} \;
  $intervals \leftarrow $  \FuncSty{searchInterestingIntervals(}$acc$,$sup$,$accTS$,$supTS$\FuncSty{)} \;
  \ForEach{$interval_i \in intervals$} {
     $newClause \leftarrow \{clause \cup lit_{new} \cup \{v \in interval_i\}\}$ \;
     Add $newClause$ to rule tree \;
  }
  $suggestions \leftarrow node.getSuggestions(clause.head)$ \;
  \ForEach{$lit_i \in suggestions$ \textbf{and} $i \leq k$} {
    $node_i \leftarrow node$.\FuncSty{search(}$lit_i$\FuncSty{)} \;
    \FuncSty{checkNumericalRanges(}$clause \cup lit_i$,$lit_{new}$,$node_i$\FuncSty{)} \;
  }
\end{algorithm}

\begin{algorithm}[!h]
 \caption{Check whether a clause has potential interesting numerical intervals}
 \label{alg5}
 \SetKwFunction{checkNumericalRanges}
 \KwIn{\textbf{Input:} $clause$: Clause with numerical literal, $lattice$: Lattice from }
  \ForEach{$lit_i \in suggestions$ \textbf{and} $i \leq k$} {
    $node_i \leftarrow node$.\FuncSty{search(}$lit_i$\FuncSty{)} \;
    \FuncSty{checkNumericalRanges(}$clause \cup lit_i$,$lit_{new}$,$node_i$\FuncSty{)} \;
  }

\end{algorithm}

In the specialization step, we can detect whether the clause contains any numerical property, which is root of a
correlation lattice. If so, it can search for interesting ranges numerical ranges, 

