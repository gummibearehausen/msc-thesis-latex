\chapter{Introduction}
\label{ch:intro}

In the last years, the volume of semantic data available, in particular in RDF format, has dramatically increased.
Initiatives like the W3C Semantic Web and the Linked Open Data  have great contribution in such development. The first
provides a common standard that allows data to be shared and reused across different applications and the latter
provides linkages between different datasets that were not originally interconnected. Moreover, advances in information
extraction have also made a strong contribution, by crawling multiple non-structured resources in the Web and extracting
RDF facts.

Nevertheless, information extraction still has its limitations and many of its sources might contain contradictory
or uncertain information. Therefore, many of the extracted datasets suffer from incompleteness, noise and uncertainty.
The firs one means that there are facts that are not existent in the dataset, the second one means that the dataset
might contain facts that are not true and the latter one means that the truth of the facts is not certain. These
problems make it much more challenging to automatically learn rules from the data.

%Moreover, for noisy and incomplete knowledge bases, the automatic (i.e., unsupervised) selection of positive and
%negative training examples needed for learning new rules poses a major challenge to the adoption of these techniques.

In order to reduce such problems, one can apply a set of inference rules that describes its domain to the knowledge
base. With that, it's possible to resolve contradictions as well as strengthen or weaken their confidence values. It's
also possible to derive new facts that are originally not existent due to incompleteness. Such inference rules can be of
two types:

\begin{enumerate}
 \item \emph{Hard Rules}: Consistency constraints which might represent functional dependencies, functional or
inverse-functional properties of predicates or mutual exclusion. For example:
    \begin{itemize}
      \item $ marriedTo(X,Y) :- marriedTo(Y,X),(X \neq Y)$
      \item $ grandChildOf(X,Y) :- childOf(X,Z),childOf(Z,Y)$
      \item $ parentOf(X,Y) :- childOf(Y,X)$
      \item $ (Z=Y) :- wasBornIn(X,Z),wasBornIn(X,Y)$
    \end{itemize}

 \item \emph{Soft Rules}: Weighted Datalog rules that frequently, but not always hold in the real world. As they
might also produce incorrect information, derived facts can have a level of uncertainty which is represented by a
confidence value. For example, it's know that married people usually live in the same place as their partner:
    \begin{center}
      $ livesIn(X,Y) :- marriedTo(X,Z)livesIn(Z,Y)$
    \end{center}
\end{enumerate}

So, if we have an incomplete knowledge base, which lacks information about where \emph{Michelle Obama} lives, but
we know that she's married to \emph{Barack Obama} and he lives in \emph{Washington, D.C.}, we could then apply this soft
rule to derive the fact \emph{livesIn(michelleObama, washingtonDC)}. 

%http://people.csail.mit.edu/kersting/profile/PROFILE_ilp.html

Such rules are rarely known beforehand, but the data itself can be used to mine these rules using Inductive Logic
Programming (ILP). It is a well-established framework for inductively learning relational descriptions (in the form of
logic programs) from examples and background knowledge. Given a logical database of facts, an ILP system will generate
hypothesis in a pre-determined order and test them against the examples. However in a large knowledge base, ILP becomes
too expensive as the search space grows combinatorially with the knowledge base size and the larger the number of
examples, the more expensive it is to test each hypothesis.

Although it is very expensive to learn rules with constants, such kind of rules can are extremely interesting as they
can express particular characteristics of specific groups. For example, the rule $speaks(X,Y)$ :- $livesIn(X,Z)$ is not
very meaningful, it simply tells that people that live in anywhere will speak some language. Nevertheless, if we
consider setting constant values for $Y$ and $Z$, we can learn much more valuable rules such as
$speaks(X,english)$ :- $livesIn(X,australia)$ or $speaks(X,spanish)$ :- $livesIn(X,mexico)$.

The problem of that, is that for such example, it would be necessary to test rules with all combinations of country and
languages, what surely can lead to a very expensive process when applied on large databases. Given the huge size of
search space and the great interestingness of rules with constants, it is appropriate to reduce the search space by
pruning constants or combinations of constants that do not add interesting information to the
hypothesis.

[Talk more about ILP]

%This will give as output a set of rules which should satisfy a minimum support and accuracy.

\section{Motivation}

For numerical properties it might be also relevant to search for interesting constants. Nevertheless, for very large or
infinite numerical domains such as the real numbers for example, setting a numerical constant as an individual value
will very likely have a single entity associated to it. In such case it would be equivalent to simply specifying a
constant for this given entity without the necessity of adding the numerical property

For example, it is extremely unlikely that a country has the exact same GDP or population as any other country. If we
have the following rule with the $hasPopulation$, whose domain is the positive integer numbers $\mathbb{N}^+$:

\begin{center}
 $speaks(X,portuguese)$ :- $livesIn(X,Z),hasPopulation(Z,193946886)$
\end{center}

As Brazil is the only country with a population of 193,946,886 inhabitants, this same rule would be equivalent to:

\begin{center}
 $speaks(X,portuguese)$ :- $livesIn(X,brazil)$
\end{center}

Of course, if we have to choose between one of the two rules, the latter one would be preferred as it is shorter and
specifies the country in a clearer manner. Moreover, by setting numerical constants punctually, we would end up having a
huge number of different constants to test in the hypothesis and most of them would be most likely discarded because of
low support.

Therefore, it is interesting to split the attribute's domain into categories by discretizing it .
Subsequently check if any of the buckets present different accuracy in comparison to its correspondent numerical
constant-free rule (which we will call base-rule).

For example if we test the hypothesis and we find support=100 and confidence=0.4:

\begin{center}
 $isMarriedTo(X,Y)$ :- $hasAge(X,Z)$ 
\end{center}

and then we split $Z$ into three buckets:

\begin{itemize}
 \item $ b_1: Z\in[0,20]$
 \item $ b_2: Z\in(20,40]$
 \item $ b_3: Z\in(40,\infty]$
\end{itemize}

we then generate three refined-rules by restricting the base-rule's domain from variable $Z$:

\begin{itemize}

 \item $isMarriedTo(X,Y) :- hasAge(X,Z), z\in[0,20]$	
    \newline support=40, confidence=0.1
 \item $isMarriedTo(X,Y) :- hasAge(X,Z), z\in(20,40]$	
    \newline support=40, confidence=0.5
 \item $isMarriedTo(X,Y) :- hasAge(X,Z), z\in(40,\infty]$
    \newline support=20, confidence=0.8

\end{itemize}

for $b_2$ and $b_3$, the hypothesis has significant gain by specifying numerical constants. Adding a relation to the
body might produce totally different support and confidence distributions along the buckets. For example, if we add the
relation $hasChild(X,A)$, we could obtain other interesting rules:

Base-rule:
\begin{itemize}
 \item $sMarriedTo(X,Y) \leftarrow hasAge(X,Z),hasChild(X,A)$	
    \newline support=50, confidence=0.625
\end{itemize}

Refined-rules:
\begin{itemize}
 \item $isMarriedTo(X,Y) \leftarrow hasAge(X,Z),hasChild(x,a), Z\in[0,20]$	
    \newline support=2, confidence=0.5
 \item $isMarriedTo(X,Y) \leftarrow hasAge(X,Z),hasChild(x,a), Z\in(20,40]$	
    \newline support=30, confidence=0.7
 \item $isMarriedTo(X,Y) \leftarrow hasAge(X,Z),hasChild(x,a), Z\in(40,\infty]$	
    \newline support=18, confidence=0.9
\end{itemize}

\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    ybar,
    enlargelimits=0.10,
    legend style={at={(0.5,-0.15)},
      anchor=north,legend columns=-1},
    ylabel={Accuracy},
    symbolic x coords={b1,b2,b3},
    xtick=data,
    nodes near coords,
    nodes near coords align={vertical},
    ]
\addplot coordinates {(b1,0.1) (b2,0.5) (b3,0.8)};
\addplot coordinates {(b1,0.5) (b2,0.7) (b3,0.9)};
\legend{isMarriedTo(X,Y):-hasAge(X,Z) , isMarriedTo(X,Y):-hasAge(X,Z)hasChild(X,A)}
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    ybar,
    enlargelimits=0.15,
    legend style={at={(0.5,-0.15)},
      anchor=north,legend columns=-1},
    ylabel={Confidence},
    symbolic x coords={b1,b2,b3},
    xtick=data,
    nodes near coords,
    nodes near coords align={vertical},
    ]
\addplot coordinates {(b1,40) (b2,40) (b3,20)};
\addplot coordinates {(b1, 2) (b2,30) (b3,18)};
\legend{isMarriedTo(X,Y):-hasAge(X,Z), isMarriedTo(X,Y):-hasAge(X,Z)hasChild(X,A)}
\end{axis}
\end{tikzpicture}

Adding a given predicate to the rule might not bring any gain or even loss in accuracy to the base-rule, but when
bucketing per age, present a different accuracy and support distribution and might even produce gain in accuracy for
some specific buckets.

Nevertheless, adding a predicate with no correlation to the rule might not generate any gain. Thus, it's necessary to
carefully choose the relations and eventual constants and discard the ones with no correlation to the rule.  

\section{Contributions}
In this thesis, we propose a pre-processing step to build a graph we call \graphname for each numerical property we want
to for interesting intervals. In each graph, that has a numerical property as root, we first query the examples
distribution on the numerical attribute, and build a histogram by splitting them into \emph{k} buckets. Subsequently, we
pick a set of \emph{c} categorical properties that can be joined with the root, extract the frequencies histogram and
analyze how the distribution of sub-population created by joining them with the root is affected. Afterwards, we try to
combine the categories and check whether they still produce interesting sub-populations creating a lattice, like in
frequent set mining apriori algorithm.

We discuss the pruning opportunities and also evaluate different heuristics and interestingness measures and their
efficiency in finding rules with numerical intervals. 

\begin{comment}
In a clause containing a numerical attribute in the body, we can obtain a support and accuracy as well as support value
for each of the buckets. Therewith, we can search the most interesting intervals that satisfies the support and accuracy
thresholds
\end{comment}

With information about different examples distributions contained in the generated lattice, once we add one of the root
properties during the core ILP algorithm, we can then search for the most interesting categorical properties that could
result in different accuracy distributions. For every categorical property we can also suggest the most interesting
constants and other categorical properties to be combined in a subcategory of both.

\section{Outline}

\begin{comment}
 The remainder of this thesis is structured as follows. In
Chapter~\ref{ch:technical_background}, we provide technical background on
MapReduce and BigTable. In Chapter~\ref{ch:related_work}, we present a
summary of previous work in the areas of duplicate and near-duplicate detection,
information retrieval on web archives, and MapReduce applications in graph
processing. Following that, we state our problem and describe solutions in
Chapter~\ref{ch:redundancy_control}. In Chapter~\ref{ch:mapreduce_impl}, we
describe an implementation of our solution using the MapReduce framework. In
Chapter~\ref{ch:experiments}, we present our experimental results. We conclude
this thesis and outline directions of future research in Chapter~\ref{ch:future_work}.
\end{comment}
